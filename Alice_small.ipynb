{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144343\n",
      "Total Vocab:  44\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144243\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.9724\n",
      "Epoch 00001: loss improved from inf to 2.97242, saving model to weights-improvement-01-2.9724.hdf5\n",
      "1127/1127 [==============================] - 979s 868ms/step - loss: 2.9724\n",
      "Epoch 2/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.7660\n",
      "Epoch 00002: loss improved from 2.97242 to 2.76601, saving model to weights-improvement-02-2.7660.hdf5\n",
      "1127/1127 [==============================] - 1086s 963ms/step - loss: 2.7660\n",
      "Epoch 3/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.6632\n",
      "Epoch 00003: loss improved from 2.76601 to 2.66320, saving model to weights-improvement-03-2.6632.hdf5\n",
      "1127/1127 [==============================] - 1051s 932ms/step - loss: 2.6632\n",
      "Epoch 4/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.5774\n",
      "Epoch 00004: loss improved from 2.66320 to 2.57736, saving model to weights-improvement-04-2.5774.hdf5\n",
      "1127/1127 [==============================] - 1078s 956ms/step - loss: 2.5774\n",
      "Epoch 5/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.5088\n",
      "Epoch 00005: loss improved from 2.57736 to 2.50878, saving model to weights-improvement-05-2.5088.hdf5\n",
      "1127/1127 [==============================] - 1108s 983ms/step - loss: 2.5088\n",
      "Epoch 6/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.4469\n",
      "Epoch 00006: loss improved from 2.50878 to 2.44687, saving model to weights-improvement-06-2.4469.hdf5\n",
      "1127/1127 [==============================] - 1028s 912ms/step - loss: 2.4469\n",
      "Epoch 7/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.3910\n",
      "Epoch 00007: loss improved from 2.44687 to 2.39098, saving model to weights-improvement-07-2.3910.hdf5\n",
      "1127/1127 [==============================] - 1072s 951ms/step - loss: 2.3910\n",
      "Epoch 8/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.3406\n",
      "Epoch 00008: loss improved from 2.39098 to 2.34065, saving model to weights-improvement-08-2.3406.hdf5\n",
      "1127/1127 [==============================] - 1109s 984ms/step - loss: 2.3406\n",
      "Epoch 9/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.2980\n",
      "Epoch 00009: loss improved from 2.34065 to 2.29797, saving model to weights-improvement-09-2.2980.hdf5\n",
      "1127/1127 [==============================] - 1118s 992ms/step - loss: 2.2980\n",
      "Epoch 10/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.2538\n",
      "Epoch 00010: loss improved from 2.29797 to 2.25376, saving model to weights-improvement-10-2.2538.hdf5\n",
      "1127/1127 [==============================] - 1066s 946ms/step - loss: 2.2538\n",
      "Epoch 11/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.2125\n",
      "Epoch 00011: loss improved from 2.25376 to 2.21248, saving model to weights-improvement-11-2.2125.hdf5\n",
      "1127/1127 [==============================] - 1017s 902ms/step - loss: 2.2125\n",
      "Epoch 12/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.1754\n",
      "Epoch 00012: loss improved from 2.21248 to 2.17539, saving model to weights-improvement-12-2.1754.hdf5\n",
      "1127/1127 [==============================] - 884s 785ms/step - loss: 2.1754\n",
      "Epoch 13/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.1352\n",
      "Epoch 00013: loss improved from 2.17539 to 2.13524, saving model to weights-improvement-13-2.1352.hdf5\n",
      "1127/1127 [==============================] - 868s 770ms/step - loss: 2.1352\n",
      "Epoch 14/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.1012\n",
      "Epoch 00014: loss improved from 2.13524 to 2.10122, saving model to weights-improvement-14-2.1012.hdf5\n",
      "1127/1127 [==============================] - 862s 765ms/step - loss: 2.1012\n",
      "Epoch 15/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.0642\n",
      "Epoch 00015: loss improved from 2.10122 to 2.06419, saving model to weights-improvement-15-2.0642.hdf5\n",
      "1127/1127 [==============================] - 866s 768ms/step - loss: 2.0642\n",
      "Epoch 16/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.0344\n",
      "Epoch 00016: loss improved from 2.06419 to 2.03438, saving model to weights-improvement-16-2.0344.hdf5\n",
      "1127/1127 [==============================] - 862s 765ms/step - loss: 2.0344\n",
      "Epoch 17/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.0014\n",
      "Epoch 00017: loss improved from 2.03438 to 2.00144, saving model to weights-improvement-17-2.0014.hdf5\n",
      "1127/1127 [==============================] - 865s 768ms/step - loss: 2.0014\n",
      "Epoch 18/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 1.9708\n",
      "Epoch 00018: loss improved from 2.00144 to 1.97078, saving model to weights-improvement-18-1.9708.hdf5\n",
      "1127/1127 [==============================] - 864s 766ms/step - loss: 1.9708\n",
      "Epoch 19/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 1.9450\n",
      "Epoch 00019: loss improved from 1.97078 to 1.94500, saving model to weights-improvement-19-1.9450.hdf5\n",
      "1127/1127 [==============================] - 865s 768ms/step - loss: 1.9450\n",
      "Epoch 20/20\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 1.9211\n",
      "Epoch 00020: loss improved from 1.94500 to 1.92107, saving model to weights-improvement-20-1.9211.hdf5\n",
      "1127/1127 [==============================] - 860s 763ms/step - loss: 1.9211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffaff1503d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
